from pinecone import Pinecone, ServerlessSpec
from gensim.models import Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import json
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv
import os
from soynlp.tokenizer import LTokenizer
from soynlp.word import WordExtractor
import re
import numpy as np

load_dotenv()
print("\nğŸ”¥ PINCONE_API_KEY ë¡œë“œë¨?:", os.getenv("PINCONE_API_KEY"))

# Pinecone ì´ˆê¸°í™”
pc = Pinecone(api_key=os.getenv("PINCONE_API_KEY"))
index_name = "bio-paper-index"

if index_name in pc.list_indexes().names():
    pc.delete_index(index_name)

pc.create_index(
    name=index_name,
    dimension=100,
    metric="cosine",
    spec=ServerlessSpec(cloud="aws", region="us-east-1")
)

index = pc.Index(index_name)

# ë°ì´í„° ë¡œë“œ
json_path = Path(__file__).parent.parent / 'data' / 'bio_research_nested.json'
with open(json_path, "r", encoding="utf-8") as f:
    data = json.load(f)

print("\nğŸ“š soynlp í† í¬ë‚˜ì´ì € í•™ìŠµ ì¤‘...")
text_corpus = [paper.get("title", "") + " " + paper.get("abstract", "")
               for user in data for paper in user["papers"]]

word_extractor = WordExtractor()
word_extractor.train(["\n".join(text_corpus)])
word_scores = word_extractor.extract()

# í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
tokenizer = LTokenizer(scores={
    k: float(v.cohesion_forward)
    for k, v in word_scores.items() if v.cohesion_forward
})

def preprocess_text(text: str):
    return tokenizer.tokenize(text.lower())

def sanitize_id(raw_id: str) -> str:
    return re.sub(r"[^a-zA-Z0-9_\-]", "_", raw_id)[:64]

print("\nğŸ› ï¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë¬¸ì„œ ì¤€ë¹„ ì¤‘...")
documents = []
for user in tqdm(data, desc="ì‚¬ìš©ì ì²˜ë¦¬ ì¤‘"):
    user_id = user["user_id"]
    for paper in user["papers"]:
        text = (paper.get("title", "") + " " + paper.get("abstract", "") +
                " ".join(paper.get("equipments", [])) + " " + " ".join(paper.get("reagents", [])))
        tokens = preprocess_text(text)
        if not tokens:
            continue
        raw_id = f"{user_id}_{paper['title'][:50].strip()}"
        doc_id = sanitize_id(raw_id)
        documents.append(TaggedDocument(tokens, [doc_id]))

print(f"âœ… ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}")

print("\nğŸ¤– Doc2Vec ëª¨ë¸ í•™ìŠµ ì¤‘...")
model = Doc2Vec(vector_size=100, min_count=2, epochs=20)
model.build_vocab(documents)
model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)

print("\nğŸ“¦ ë²¡í„° ìƒì„± ë° ì—…ë¡œë“œ ì¤‘...")
vectors_to_upsert = []
vector_count = 0
for doc in documents:
    doc_id = doc.tags[0]
    vector = model.dv[doc_id]
    # ë©”íƒ€ë°ì´í„° ì¶”ì 
    for user in data:
        for paper in user["papers"]:
            if doc_id.startswith(f"{user['user_id']}_"):
                metadata = {
                    "user_id": user["user_id"],
                    "title": paper.get("title", ""),
                    "abstract": paper.get("abstract", ""),
                    "year": paper.get("year", ""),
                    "journal": paper.get("journal", ""),
                    "equipments": paper.get("equipments", []),
                    "reagents": paper.get("reagents", [])
                }
                vectors_to_upsert.append({
                    "id": doc_id,
                    "values": vector.tolist(),
                    "metadata": metadata
                })
                vector_count += 1
                break

    if len(vectors_to_upsert) >= 100:
        index.upsert(vectors=vectors_to_upsert)
        vectors_to_upsert = []

if vectors_to_upsert:
    index.upsert(vectors=vectors_to_upsert)

print(f"\nâœ… ì´ {vector_count}ê°œì˜ ë²¡í„°ê°€ Pineconeì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
print("\nğŸ§  ì¸ë±ìŠ¤ ìƒíƒœ:", index.describe_index_stats())

def get_recommendations(query_text, top_k=5):
    tokens = preprocess_text(query_text)
    print(f"\nğŸ” ì¿¼ë¦¬: '{query_text}' â†’ í† í°: {tokens}")
    if not tokens:
        print("ğŸš¨ ìœ íš¨í•œ í† í° ì—†ìŒ!")
        return []

    query_vector = model.infer_vector(tokens, epochs=50)
    print(f"ğŸ§ª ë²¡í„° í‰ê· : {np.mean(query_vector):.4f}, ë¶„ì‚°: {np.var(query_vector):.4f}")

    results = index.query(vector=query_vector.tolist(), top_k=top_k, include_metadata=True)

    print(f"ğŸ“Š ìœ ì‚¬í•œ ë¬¸ì„œ {len(results.matches)}ê°œ ë°œê²¬")
    recommended_users = []
    for match in results.matches:
        user_id = match.metadata.get("user_id")
        print(f"âœ… ì¶”ì²œ: {user_id} | ì œëª©: {match.metadata.get('title')} | score: {match.score:.4f}")
        if user_id not in recommended_users:
            recommended_users.append(user_id)

    return recommended_users

if __name__ == "__main__":
    queries = ["ì‹ ê²½ì„¸í¬", "ì¤„ê¸°ì„¸í¬", "ì„¸í¬", "ë©´ì—­", "ë‹¨ë°±ì§ˆ", "DNA"]
    for q in queries:
        print("\n" + "="*60)
        print(f"ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸: {q}")
        result = get_recommendations(q)
        print(f"ğŸ”— ì¶”ì²œëœ ì‚¬ìš©ì: {result}")